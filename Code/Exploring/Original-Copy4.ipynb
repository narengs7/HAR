{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility layer between Python 2 and Python 3\n",
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(dataset):\n",
    "\n",
    "    mu = np.mean(dataset, axis=0)\n",
    "    sigma = np.std(dataset, axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "\n",
    "def show_confusion_matrix(validations, predictions):\n",
    "\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap=\"coolwarm\",\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt=\"d\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_basic_dataframe_info(dataframe,\n",
    "                              preview_rows=20):\n",
    "\n",
    "    \"\"\"\n",
    "    This function shows basic information for the given dataframe\n",
    "    Args:\n",
    "        dataframe: A Pandas DataFrame expected to contain data\n",
    "        preview_rows: An integer value of how many rows to preview\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # Shape and how many rows and columns\n",
    "    print(\"Number of columns in the dataframe: %i\" % (dataframe.shape[1]))\n",
    "    print(\"Number of rows in the dataframe: %i\\n\" % (dataframe.shape[0]))\n",
    "    print(\"First 20 rows of the dataframe:\\n\")\n",
    "    # Show first 20 rows\n",
    "    print(dataframe.head(preview_rows))\n",
    "    print(\"\\nDescription of dataframe:\\n\")\n",
    "    # Describe dataset like mean, min, max, etc.\n",
    "    # print(dataframe.describe())\n",
    "\n",
    "\n",
    "def read_data(file_path):\n",
    "\n",
    "    \"\"\"\n",
    "    This function reads the accelerometer data from a file\n",
    "    Args:\n",
    "        file_path: URL pointing to the CSV file\n",
    "    Returns:\n",
    "        A pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    column_names = ['user-id',\n",
    "                    'activity',\n",
    "                    'timestamp',\n",
    "                    'x-axis',\n",
    "                    'y-axis',\n",
    "                    'z-axis']\n",
    "    df = pd.read_csv(file_path,\n",
    "                     header=None,\n",
    "                     names=column_names)\n",
    "    # Last column has a \";\" character which must be removed ...\n",
    "    df['z-axis'].replace(regex=True,\n",
    "      inplace=True,\n",
    "      to_replace=r';',\n",
    "      value=r'')\n",
    "    # ... and then this column must be transformed to float explicitly\n",
    "    df['z-axis'] = df['z-axis'].apply(convert_to_float)\n",
    "    # This is very important otherwise the model will not fit and loss\n",
    "    # will show up as NAN\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_float(x):\n",
    "\n",
    "    try:\n",
    "        return np.float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Not used right now\n",
    "def feature_normalize(dataset):\n",
    "\n",
    "    mu = np.mean(dataset, axis=0)\n",
    "    sigma = np.std(dataset, axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "\n",
    "def plot_axis(ax, x, y, title):\n",
    "\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "def plot_activity(activity, data):\n",
    "\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows=3,\n",
    "         figsize=(15, 10),\n",
    "         sharex=True)\n",
    "    plot_axis(ax0, data['timestamp'], data['x-axis'], 'x-axis')\n",
    "    plot_axis(ax1, data['timestamp'], data['y-axis'], 'y-axis')\n",
    "    plot_axis(ax2, data['timestamp'], data['z-axis'], 'z-axis')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(activity)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_segments_and_labels(df, time_steps, step, label_name):\n",
    "\n",
    "    \"\"\"\n",
    "    This function receives a dataframe and returns the reshaped segments\n",
    "    of x,y,z acceleration as well as the corresponding labels\n",
    "    Args:\n",
    "        df: Dataframe in the expected format\n",
    "        time_steps: Integer value of the length of a segment that is created\n",
    "    Returns:\n",
    "        reshaped_segments\n",
    "        labels:\n",
    "    \"\"\"\n",
    "\n",
    "    # x, y, z acceleration as features\n",
    "    N_FEATURES = 3\n",
    "    # Number of steps to advance in each iteration (for me, it should always\n",
    "    # be equal to the time_steps in order to have no overlap between segments)\n",
    "    # step = time_steps\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - time_steps, step):\n",
    "        xs = df['x-axis'].values[i: i + time_steps]\n",
    "        ys = df['y-axis'].values[i: i + time_steps]\n",
    "        zs = df['z-axis'].values[i: i + time_steps]\n",
    "        # Retrieve the most often used label in this segment\n",
    "        label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n",
    "        segments.append([xs, ys, zs])\n",
    "        labels.append(label)\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return reshaped_segments, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- THE PROGRAM TO LOAD DATA AND TRAIN THE MODEL -------\n",
    "\n",
    "# Set some standard parameters upfront\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "sns.set() # Default seaborn look and feel\n",
    "plt.style.use('ggplot')\n",
    "print('keras version ', keras.__version__)\n",
    "\n",
    "LABELS = [\"Downstairs\",\n",
    "          \"Jogging\",\n",
    "          \"Sitting\",\n",
    "          \"Standing\",\n",
    "          \"Upstairs\",\n",
    "          \"Walking\"]\n",
    "# The number of steps within one time segment\n",
    "TIME_PERIODS = 200#80\n",
    "# The steps to take from one segment to the next; if this value is equal to\n",
    "# TIME_PERIODS, then there is no overlap between the segments\n",
    "STEP_DISTANCE = 100#40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data('../Data/WISDM_ar_v1.1_raw.txt')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_basic_dataframe_info(df, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['activity'].value_counts().plot(kind='bar',\n",
    "                                   title='Training Examples by Activity Type')\n",
    "plt.show()\n",
    "\n",
    "df['user-id'].value_counts().plot(kind='bar',\n",
    "                                  title='Training Examples by User')\n",
    "plt.show()\n",
    "\n",
    "for activity in np.unique(df[\"activity\"]):\n",
    "    subset = df[df[\"activity\"] == activity][100:280]\n",
    "    plot_activity(activity, subset)\n",
    "\n",
    "# Define column name of the label vector\n",
    "LABEL = \"ActivityEncoded\"\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "df[LABEL] = le.fit_transform(df[\"activity\"].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['user-id']<=700]['user-id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Reshape the data into segments ---\\n\")\n",
    "\n",
    "# Differentiate between test set and training set\n",
    "df_test = df[df['user-id'] > 28]\n",
    "df_train = df[df['user-id'] <= 28]\n",
    "\n",
    "# Normalize features for training data set\n",
    "df_train['x-axis'] = feature_normalize(df['x-axis'])\n",
    "df_train['y-axis'] = feature_normalize(df['y-axis'])\n",
    "df_train['z-axis'] = feature_normalize(df['z-axis'])\n",
    "# Round in order to comply to NSNumber from iOS\n",
    "df_train = df_train.round({'x-axis': 6, 'y-axis': 6, 'z-axis': 6})\n",
    "\n",
    "# Reshape the training data into segments\n",
    "# so that they can be processed by the network\n",
    "x_train, y_train = create_segments_and_labels(df_train,\n",
    "                                              TIME_PERIODS,\n",
    "                                              STEP_DISTANCE,\n",
    "                                              LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Reshape data to be accepted by Keras ---\\n\")\n",
    "\n",
    "# Inspect x data\n",
    "print('x_train shape: ', x_train.shape)\n",
    "# Displays (20869, 40, 3)\n",
    "print(x_train.shape[0], 'training samples')\n",
    "# Displays 20869 train samples\n",
    "\n",
    "# Inspect y data\n",
    "print('y_train shape: ', y_train.shape)\n",
    "# Displays (20869,)\n",
    "\n",
    "# Set input & output dimensions\n",
    "num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
    "num_classes = le.classes_.size\n",
    "print(list(le.classes_))\n",
    "\n",
    "# Set input_shape / reshape for Keras\n",
    "# Remark: acceleration data is concatenated in one array in order to feed\n",
    "# it properly into coreml later, the preferred matrix of shape [40,3]\n",
    "# cannot be read in with the current version of coreml (see also reshape\n",
    "# layer as the first layer in the keras model)\n",
    "input_shape = (num_time_periods*num_sensors)\n",
    "# x_train = x_train.reshape(x_train.shape[0], input_shape)\n",
    "\n",
    "\n",
    "# Convert type for Keras otherwise Keras cannot process the data\n",
    "x_train = x_train.astype(\"float32\")\n",
    "y_train = y_train.astype(\"float32\")\n",
    "\n",
    "# %%\n",
    "\n",
    "# One-hot encoding of y_train labels (only execute once!)\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "print('New y_train shape: ', y_train.shape)\n",
    "# (4173, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- Create neural network model ---\\n\")\n",
    "\n",
    "# # 1D CNN neural network\n",
    "# model_m = Sequential()\n",
    "# model_m.add(Conv1D(130, 3, activation='relu', input_shape=(TIME_PERIODS, num_sensors)))\n",
    "# # model_m.add(BatchNormalization())\n",
    "# model_m.add(Conv1D(130, 3, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(BatchNormalization())\n",
    "# model_m.add(Conv1D(150, 3, activation='relu'))\n",
    "# # model_m.add(BatchNormalization())\n",
    "\n",
    "# model_m.add(Conv1D(150, 3, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(BatchNormalization())\n",
    "\n",
    "# model_m.add(Conv1D(170, 3, activation='relu'))\n",
    "# # model_m.add(BatchNormalization())\n",
    "\n",
    "# model_m.add(Conv1D(170, 3, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(BatchNormalization())\n",
    "\n",
    "# # model_m.add(Conv1D(190, 3, activation='relu'))\n",
    "# # model_m.add(Dropout(0.5))\n",
    "# # model_m.add(Conv1D(22, 2, activation='relu'))\n",
    "# # model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(Conv1D(24, 2, activation='relu'))\n",
    "# # model_m.add(Conv1D(24, 2, activation='relu'))\n",
    "# # model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(Conv1D(180, 7, activation='relu'))\n",
    "# # model_m.add(Conv1D(180, 7, activation='relu'))\n",
    "# # model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(Conv1D(80, 10, activation='relu'))\n",
    "# # model_m.add(Conv1D(80, 10, activation='relu'))\n",
    "# model_m.add(GlobalAveragePooling1D())\n",
    "# model_m.add(Dropout(0.5))\n",
    "# model_m.add(Dense(num_classes, activation='softmax'))\n",
    "# print(model_m.summary())\n",
    "# # Accuracy on training data: 99%\n",
    "# # Accuracy on test data: 91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- Create neural network model ---\\n\")\n",
    "\n",
    "# # 1D CNN neural network\n",
    "# model_m = Sequential()\n",
    "# model_m.add(Reshape((TIME_PERIODS, num_sensors), input_shape=(input_shape,)))\n",
    "# model_m.add(Conv1D(130, 4, activation='relu', input_shape=(TIME_PERIODS, num_sensors)))\n",
    "# # model_m.add(BatchNormalization())\n",
    "# model_m.add(Conv1D(130, 4, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(BatchNormalization())\n",
    "# model_m.add(Conv1D(150, 4, activation='relu'))\n",
    "# # model_m.add(BatchNormalization())\n",
    "\n",
    "# model_m.add(Conv1D(150, 4, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(BatchNormalization())\n",
    "\n",
    "# model_m.add(Conv1D(170, 4, activation='relu'))\n",
    "# # model_m.add(BatchNormalization())\n",
    "\n",
    "# # model_m.add(Conv1D(170, 4, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(BatchNormalization())\n",
    "\n",
    "# # model_m.add(Conv1D(190, 3, activation='relu'))\n",
    "# # model_m.add(Dropout(0.5))\n",
    "# # model_m.add(Conv1D(22, 2, activation='relu'))\n",
    "# # model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(Conv1D(24, 2, activation='relu'))\n",
    "# # model_m.add(Conv1D(24, 2, activation='relu'))\n",
    "# # model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(Conv1D(180, 7, activation='relu'))\n",
    "# # model_m.add(Conv1D(180, 7, activation='relu'))\n",
    "# # model_m.add(MaxPooling1D(3))\n",
    "# # model_m.add(Conv1D(80, 10, activation='relu'))\n",
    "# # model_m.add(Conv1D(80, 10, activation='relu'))\n",
    "# model_m.add(GlobalAveragePooling1D())\n",
    "# model_m.add(Dropout(0.5))\n",
    "# model_m.add(Dense(num_classes, activation='softmax'))\n",
    "# print(model_m.summary())\n",
    "# # Accuracy on training data: 99%\n",
    "# # Accuracy on test data: 91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Create neural network model ---\\n\")\n",
    "\n",
    "# 1D CNN neural network\n",
    "model_m = Sequential()\n",
    "# model_m.add(Reshape((TIME_PERIODS, num_sensors), input_shape=(input_shape,)))\n",
    "model_m.add(Conv1D(20, 5, activation='relu', input_shape=(TIME_PERIODS, num_sensors)))\n",
    "# model_m.add(BatchNormalization())\n",
    "model_m.add(Conv1D(20, 5, activation='relu'))\n",
    "model_m.add(MaxPooling1D(3))\n",
    "# model_m.add(BatchNormalization())\n",
    "model_m.add(Conv1D(50, 5, activation='relu'))\n",
    "model_m.add(BatchNormalization())\n",
    "\n",
    "model_m.add(Conv1D(50, 5, activation='relu'))\n",
    "model_m.add(MaxPooling1D(3))\n",
    "model_m.add(BatchNormalization())\n",
    "\n",
    "model_m.add(Conv1D(70, 5, activation='relu'))\n",
    "model_m.add(BatchNormalization())\n",
    "\n",
    "# model_m.add(Conv1D(170, 4, activation='relu'))\n",
    "model_m.add(MaxPooling1D(3))\n",
    "\n",
    "# model_m.add(Conv1D(190, 3, activation='relu'))\n",
    "# model_m.add(Dropout(0.5))\n",
    "# model_m.add(Conv1D(22, 2, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# model_m.add(Conv1D(24, 2, activation='relu'))\n",
    "# model_m.add(Conv1D(24, 2, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# model_m.add(Conv1D(180, 7, activation='relu'))\n",
    "# model_m.add(Conv1D(180, 7, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# model_m.add(Conv1D(80, 10, activation='relu'))\n",
    "# model_m.add(Conv1D(80, 10, activation='relu'))\n",
    "model_m.add(GlobalAveragePooling1D())\n",
    "# model_m.add(Dropout(0.5))\n",
    "model_m.add(Dense(num_classes, activation='softmax'))\n",
    "print(model_m.summary())\n",
    "# Accuracy on training data: 99%\n",
    "# Accuracy on test data: 91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- Create neural network model ---\\n\")\n",
    "\n",
    "# # 1D CNN neural network\n",
    "# model_m = Sequential()\n",
    "# model_m.add(Conv1D(200, 5, activation='relu', input_shape=(TIME_PERIODS, num_sensors)))\n",
    "# # model_m.add(BatchNormalization())\n",
    "# model_m.add(Conv1D(160, 5, activation='relu'))\n",
    "# model_m.add(Conv1D(160, 5, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "\n",
    "# # model_m.add(BatchNormalization())\n",
    "# model_m.add(Conv1D(140, 5, activation='relu'))\n",
    "# model_m.add(Conv1D(140, 5, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "\n",
    "# model_m.add(Conv1D(100, 5, activation='relu'))\n",
    "# model_m.add(Conv1D(100, 5, activation='relu'))\n",
    "\n",
    "# model_m.add(Conv1D(80, 5, activation='relu'))\n",
    "# # model_m.add(Conv1D(170, 4, activation='relu'))\n",
    "# model_m.add(MaxPooling1D(3))\n",
    "# model_m.add(Flatten())\n",
    "# # model_m.add(Dropout(0.5))\n",
    "# model_m.add(Dense(num_classes, activation='softmax'))\n",
    "# print(model_m.summary())\n",
    "# # Accuracy on training data: 99%\n",
    "# # Accuracy on test data: 91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Fit the model ---\\n\")\n",
    "from tensorflow.keras import optimizers\n",
    "# The EarlyStopping callback monitors training accuracy:\n",
    "# if it fails to improve for two consecutive epochs,\n",
    "# training stops early\n",
    "callbacks_list = [\n",
    "#     keras.callbacks.ModelCheckpoint(\n",
    "#         filepath='best_model_91_BN_3.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "#         monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='accuracy', patience=1)\n",
    "]\n",
    "\n",
    "adam = optimizers.Adam(lr=0.0001)\n",
    "\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer=adam, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "EPOCHS = 50\n",
    "\n",
    "# Enable validation to use ModelCheckpoint and EarlyStopping callbacks.\n",
    "history = model_m.fit(x_train,\n",
    "                      y_train,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Learning curve of model training ---\\n\")\n",
    "\n",
    "# summarize history for accuracy and loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['accuracy'], \"g--\", label=\"Accuracy of training data\")\n",
    "plt.plot(history.history['val_accuracy'], \"g\", label=\"Accuracy of validation data\")\n",
    "plt.plot(history.history['loss'], \"r--\", label=\"Loss of training data\")\n",
    "plt.plot(history.history['val_loss'], \"r\", label=\"Loss of validation data\")\n",
    "plt.title('Model Accuracy and Loss')\n",
    "plt.ylabel('Accuracy and Loss')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Check against test data ---\\n\")\n",
    "\n",
    "# Normalize features for training data set\n",
    "df_test['x-axis'] = feature_normalize(df_test['x-axis'])\n",
    "df_test['y-axis'] = feature_normalize(df_test['y-axis'])\n",
    "df_test['z-axis'] = feature_normalize(df_test['z-axis'])\n",
    "\n",
    "df_test = df_test.round({'x-axis': 6, 'y-axis': 6, 'z-axis': 6})\n",
    "\n",
    "x_test, y_test = create_segments_and_labels(df_test,\n",
    "                                            TIME_PERIODS,\n",
    "                                            STEP_DISTANCE,\n",
    "                                            LABEL)\n",
    "\n",
    "# Set input_shape / reshape for Keras\n",
    "x_test = x_test.reshape(x_test.shape[0], input_shape)\n",
    "\n",
    "x_test = x_test.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "score = model_m.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
    "print(\"\\nLoss on test data: %0.2f\" % score[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Confusion matrix for test data ---\\n\")\n",
    "\n",
    "y_pred_test = model_m.predict(x_test)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "max_y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_test, max_y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Classification report for test data ---\\n\")\n",
    "\n",
    "print(classification_report(max_y_test, max_y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1_score(max_y_test, max_y_pred_test,pos_label='positive',average='micro'),precision_score(max_y_test, max_y_pred_test,pos_label='positive',average='micro'),recall_score(max_y_test, max_y_pred_test,pos_label='positive',average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2 = read_data('../Data/WISDM_at_v2.0_raw.txt')\n",
    "df_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2 = df_v2[df_v2['activity'].isin([ 'Walking'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_basic_dataframe_info(df_v2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2[LABEL] = le.fit_transform(df_v2[\"activity\"].values.ravel())\n",
    "df_v2['x-axis'] = feature_normalize(df_v2['x-axis'])\n",
    "df_v2['y-axis'] = feature_normalize(df_v2['y-axis'])\n",
    "df_v2['z-axis'] = feature_normalize(df_v2['z-axis'])\n",
    "df_v2 = df_v2.round({'x-axis': 6, 'y-axis': 6, 'z-axis': 6})\n",
    "\n",
    "x_test_v2, y_test_v2 = create_segments_and_labels(df_v2,\n",
    "                                            TIME_PERIODS,\n",
    "                                            STEP_DISTANCE,\n",
    "                                            LABEL)\n",
    "\n",
    "# Set input_shape / reshape for Keras\n",
    "x_test_v2 = x_test_v2.reshape(x_test_v2.shape[0], input_shape)\n",
    "\n",
    "x_test_v2 = x_test_v2.astype(\"float32\")\n",
    "y_test_v2 = y_test_v2.astype(\"float32\")\n",
    "\n",
    "y_test_v2 = np_utils.to_categorical(y_test_v2, num_classes)\n",
    "\n",
    "score = model_m.evaluate(x_test_v2, y_test_v2, verbose=1)\n",
    "\n",
    "print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
    "print(\"\\nLoss on test data: %0.2f\" % score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Confusion matrix for test data ---\\n\")\n",
    "\n",
    "y_pred_test_v2 = model_m.predict(x_test_v2)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test_v2 = np.argmax(y_pred_test_v2, axis=1)\n",
    "max_y_test_v2 = np.argmax(y_test_v2, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_test_v2, max_y_pred_test_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
